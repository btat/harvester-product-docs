= StorageClass

A StorageClass allows administrators to describe the *classes* of storage they offer. Different {longhorn-product-name} StorageClasses might map to replica policies, or to node schedule policies, or disk schedule policies determined by the cluster administrators. This concept is sometimes called *profiles* in other storage systems.

[NOTE]
====
For information about leveraging external storage, see xref:./csidriver.adoc[Third-Party Storage Support].
====

== Creating a StorageClass

[CAUTION]
====
Once the StorageClass is created, you can only edit the description. All other settings are fixed.
====

. Go to *Advanced -> StorageClasses*.
+
image::storageclass/create_storageclasses_entry.png[]

. In the general information section, configure the following settings:
+
* *Name*: Name of the StorageClass.
* *Description* (optional): Description of the StorageClass.
* *Provisioner*: Provisioner that determines the volume plugin to be used for provisioning volumes.

. On the *Parameters* tab, configure the following settings:
+
* *Number of Replicas*: Number of replicas created for each {longhorn-product-name} volume. The default value is `3`. 
* *Stale Replica Timeout*: Number of minutes {longhorn-product-name} waits before cleaning up a replica with the status `ERROR`. The default value is `30`.
* *Node Selector* (optional): Node tags to be matched during volume scheduling. You can add node tags on the host configuration screen (*Host -> Edit Config*).
* *Disk Selector* (optional): Disk tags to be matched during volume scheduling. You can add disk tags on the host configuration screen (*Host -> Edit Config*).
* *Migratable*: Whether xref:../virtual-machines/live-migration.adoc[Live Migration] is supported. The default value is `Yes`.

. On the *Customize* tab, configure the following settings:
+
* *Reclaim Policy*: Volumes dynamically created by a StorageClass have the reclaim policy specified in the *Reclaim Policy* field of the StorageClass. The default value is `Delete`.
** *Delete*: Deletes volumes and the underlying devices when the volume claim is deleted.
** *Retain*: Retains the volume for manual cleanup.
+
* *Allow Volume Expansion*: Volumes can be configured to be expandable. The default value is `Enabled`, which allows you to resize the volume by editing the corresponding PVC object.
+
[NOTE]
====
You can only use the volume expansion feature to increase the volume size.
====
+
* *Volume Binding Mode*: You can specify when volume binding and dynamic provisioning should occur. The default value is `Immediate`.
** *Immediate*: Binds and provisions a volume once the PVC is created.
** *WaitForFirstConsumer*: Binds and provisions a volume once a virtual machine using the PVC is created. 

. Click *Create*.

== Data Locality Settings

You can use the `dataLocality` parameter when at least one replica of a {longhorn-product-name} volume must be scheduled on the same node as the pod that uses the volume (whenever possible).

{harvester-product-name} officially supports data locality. This applies even to volumes created from xref:../virtual-machines/vm-images/upload-image.adoc[images]. To configure data locality, create a new StorageClass on the {harvester-product-name} UI (*Storage Classess -> Create -> Parameters*) and then add the following parameter:

* *Key*: `dataLocality`
* *Value*: `disabled` or `best-effort`

image::storageclass/data-locality.png[]

=== Data Locality Options

{harvester-product-name} currently supports the following options:

* `disabled`: When applied, {longhorn-product-name} may or may not schedule a replica on the same node as the pod that uses the volume. This is the default option.
* `best-effort`: When applied, {longhorn-product-name} always attempts to schedule a replica on the same node as the pod that uses the volume. {longhorn-product-name} does not stop the volume even when a local replica is unavailable because of an environmental limitation (for example, insufficient disk space or incompatible disk tags).

[NOTE]
====
{longhorn-product-name} provides a third option called `strict-local`, which forces {longhorn-product-name} to keep only one replica on the same node as the pod that uses the volume. {harvester-product-name} does not support this option because it can affect certain operations such as xref:../virtual-machines/live-migration.adoc[VM Live Migration].
====

For more information, see https://documentation.suse.com/cloudnative/storage/1.7/en/high-availability/data-locality.html[Data Locality] in the {longhorn-product-name} documentation.

== Appendix - Use Case

=== HDD Scenario

With the introduction of _StorageClass_, users can now use *HDDs* for tiered or archived cold storage.

[CAUTION]
====
HDD is not recommended for guest RKE2 clusters or VMs with good performance disk requirements.
====

==== Recommended Practice

First, add your HDD on the `Host` page and specify the disk tags as needed, such as `HDD` or `ColdStorage`. For more information on how to add extra disks and disk tags, see xref:../hosts/hosts.adoc#_multi_disk_management[Multi-disk Management] for details.

image::storageclass/add_hdd_on_host_page.png[]

image::storageclass/add_tags.png[]

Then, create a new `StorageClass` for the HDD (use the above disk tags). For hard drives with large capacity but slow performance, the number of replicas can be reduced to improve performance.

image::storageclass/create_hdd_storageclass.png[]

You can now create a volume using the above `StorageClass` with HDDs mostly for cold storage or archiving purpose.

image::storageclass/create_volume_hdd.png[]
