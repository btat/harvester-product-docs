= Upgrades
:revdate: 2025-06-18
:page-revdate: {revdate}

== Upgrade support matrix

The following table shows the upgrade path of all supported versions.

|===
| Upgrade from version | Supported new version(s)

| xref:./v1-4-1-to-v1-4-3.adoc[v1.4.1, v1.4.2]
| v1.4.3

| xref:./v1-4-1-to-v1-4-2.adoc[v1.4.1]
| v1.4.2

| xref:./v1-4-0-to-v1-4-1.adoc[v1.4.0]
| v1.4.1

| xref:./v1-3-1-to-v1-3-2.adoc[v1.3.1]
| v1.3.2

| xref:./v1-2-2-to-v1-3-1.adoc[v1.2.2/v1.3.0]
| v1.3.1

| xref:./v1-2-1-to-v1-2-2.adoc[v1.2.1]
| v1.2.2

| xref:./v1-2-0-to-v1-2-1.adoc[v1.1.2/v1.1.3/v1.2.0]
| v1.2.1
|===

== {rancher-short-name} upgrade

If you are using {rancher-short-name} to manage your {harvester-product-name} cluster, we recommend upgrading your {rancher-short-name} server first. For more information, please refer to the https://documentation.suse.com/cloudnative/rancher-manager/v2.10/en/installation-and-upgrade/upgrades.html[{rancher-short-name} upgrade guide].

For the {harvester-product-name} & {rancher-short-name} support matrix, please visit our website https://www.suse.com/suse-harvester/support-matrix/all-supported-versions/[here].

[NOTE]
====
* Upgrading {rancher-short-name} will not automatically upgrade your {harvester-product-name} cluster. You still need to upgrade your {harvester-product-name} cluster after upgrading {rancher-short-name}.
* Upgrading {rancher-short-name} will not bring your {harvester-product-name} cluster down. You can still access your {harvester-product-name} cluster using its virtual IP.
====

== Before starting an upgrade

Check out the available xref:../installation-setup/config/settings.adoc#_upgrade_config[`upgrade-config` setting] to tweak the upgrade strategies and behaviors that best suit your cluster environment.

== Start an upgrade

[CAUTION]
====
* Before you upgrade your {harvester-product-name} cluster, we highly recommend:
 ** Back up your VMs if needed.
* Do not operate the cluster during an upgrade. For example, creating new VMs, uploading new images, etc.
* Make sure your hardware meets the *preferred* xref:../installation-setup/requirements.adoc#_hardware_requirements[hardware requirements]. This is due to there will be intermediate resources consumed by an upgrade.
* Make sure each node has at least 30 GiB of free system partition space (`df -h /usr/local/`). If any node in the cluster has less than 30 GiB of free system partition space, the upgrade will be denied. Check <<Free system partition space requirement,free system partition space requirement>> for more information.
* Run the pre-check script on a {harvester-product-name} control-plane node. Please pick a script according to your cluster's version: https://github.com/harvester/upgrade-helpers/tree/main/pre-check.
* A number of one-off privileged pods will be created in the `harvester-system` and `cattle-system` namespaces to perform host-level upgrade operations. If https://kubernetes.io/docs/concepts/security/pod-security-admission/[pod security admission] is enabled, adjust these policies to allow these pods to run.
====

[CAUTION]
====
* Make sure all nodes' times are in sync. Using an NTP server to synchronize time is recommended. If an NTP server is not configured during the installation, you can manually add an NTP server *on each node*:
+
[,sh]
----
  $ sudo -i

  # Add time servers
  $ vim /etc/systemd/timesyncd.conf
  [ntp]
  NTP=0.pool.ntp.org

  # Enable and start the systemd-timesyncd
  $ timedatectl set-ntp true

  # Check status
  $ sudo timedatectl status
----
====

[CAUTION]
====
* NICs that connect to a PCI bridge might be renamed after an upgrade. Please check the https://harvesterhci.io/kb/nic-naming-scheme[knowledge base article] for further information.
====

* Make sure to read the Warning paragraph at the top of this document first.
* {harvester-product-name} checks if there are new upgradable versions periodically. If there are new versions, an upgrade button shows up on the Dashboard screen.
 ** If the cluster is in an air-gapped environment, please see <<Prepare an air-gapped upgrade>> section first. You can also speed up the ISO download by using the approach in that section.
* Navigate to {harvester-product-name} GUI and click the upgrade button on the Dashboard screen.
+
image::upgrade/upgrade_button.png[]

* Select a version to start upgrading.
+
image::upgrade/upgrade_select_version.png[]

* Click the circle on the top to display the upgrade progress.
+
image:upgrade/upgrade_progress.png[]

== Prepare an air-gapped upgrade

[CAUTION]
====
Make sure to check <<Upgrade support matrix>> section first about upgradable versions.
====

* Download an ISO file from the https://github.com/harvester/harvester/releases[Releases] page.
* Save the ISO to a local HTTP server. Assume the file is hosted at `+http://10.10.0.1/harvester.iso+`.
* Download the version file from the Releases page (for example, `+https://releases.rancher.com/harvester/{version}/version.yaml+`).
 ** Replace `isoURL` value in the `version.yaml` file:
+
[,yaml]
----
  apiVersion: harvesterhci.io/v1beta1
  kind: Version
  metadata:
    name: v1.0.2
    namespace: harvester-system
  spec:
    isoChecksum: <SHA-512 checksum of the ISO>
    isoURL: http://10.10.0.1/harvester.iso  # change to local ISO URL
    releaseDate: '20220512'
----
 ** Assume the file is hosted at `+http://10.10.0.1/version.yaml+`.
* Log in to one of your control plane nodes.
* Become root and create a version:
+
[,sh]
----
  rancher@node1:~> sudo -i
  rancher@node1:~> kubectl create -f http://10.10.0.1/version.yaml
----

* An upgrade button should show up on the {harvester-product-name} UI Dashboard screen.

== Free system partition space requirement

{harvester-product-name} checks the amount of free system partition space on each node when you select *Upgrade*. If any node does not meet the requirement, the upgrade is denied as follows:

image::upgrade/upgrade_free_space_check.png[]

If you want to try upgrading even if the free system partition space is insufficient on some nodes, you can update the `harvesterhci.io/minFreeDiskSpaceGB` annotation of the `Version` object.

[,yaml]
----
apiVersion: harvesterhci.io/v1beta1
kind: Version
metadata:
  annotations:
    harvesterhci.io/minFreeDiskSpaceGB: "30" # the value is pre-defined and may be customized
  name: 1.2.0
  namespace: harvester-system
spec:
  isoChecksum: <SHA-512 checksum of the ISO>
  isoURL: http://192.168.0.181:8000/harvester-master-amd64.iso
  minUpgradableVersion: 1.1.2
  releaseDate: "20230609"
----

[CAUTION]
====
Setting a smaller value than the pre-defined value may cause the upgrade to fail and is not recommended in a production environment.
====

The following sections describe solutions for issues related to this requirement.

=== Free system partition space manually

{harvester-product-name} attempts to remove unnecessary container images after an upgrade is completed. However, this automatic image cleanup may not be performed for various reasons. You can use https://github.com/harvester/upgrade-helpers/blob/main/bin/harv-purge-images.sh[a script] to manually remove images. For more information, see issue https://github.com/harvester/harvester/issues/6620[#6620].

=== Set up a private container registry and skip image preloading

The system partition might still lack free space even after you remove images. To address this, set up a private container registry for both current and new images, and configure the setting xref:../installation-setup/config/settings.adoc#_upgrade_config[`upgrade-config`] with following value:

[,json]
----
{"imagePreloadOption":{"strategy":{"type":"skip"}}, "restoreVM": false}
----

{harvester-product-name} skips the upgrade image preloading process. When the deployments on the nodes are upgraded, the container runtime loads the images stored in the private container registry.

[CAUTION]
====
Do not rely on the public container registry. Note any potential internet service interruptions and how close you are to reaching your https://www.docker.com/increase-rate-limits[Docker Hub rate limit]. Failure to download any of the required images may cause the upgrade to fail and may leave the cluster in a middle state.
====

== Longhorn Manager Crashes Due to Backing Image Eviction

[CAUTION]
====
When upgrading to {harvester-product-name} *v1.4.x*, Longhorn Manager may crash if the `EvictionRequested` flag is set to `true` on any node or disk. This issue is caused by a https://longhorn.io/kb/troubleshooting-longhorn-manager-crashes-due-to-backing-image-eviction/[race condition] between the deletion of a disk in the backing image spec and the updating of its status.

To prevent the issue from occurring, ensure that the `EvictionRequested` flag is set to `false` before you start the upgrade process.
====

== Re-enable RKE2 ingress-nginx admission webhooks (CVE-2025-1974)

If you https://harvesterhci.io/kb/2025/03/25/cve-2025-1974[disabled the RKE2 ingress-nginx admission webhooks] to mitigate https://nvd.nist.gov/vuln/detail/CVE-2025-1974[CVE-2025-1974], you must re-enable the webhook after upgrading to {harvester-product-name} v1.5.0 or later.

. Verify that {harvester-product-name} is using nginx-ingress v1.12.1 or later.
+
[,shell]
----
$ kubectl -n kube-system get po -l"app.kubernetes.io/name=rke2-ingress-nginx" -ojsonpath='{.items[].spec.containers[].image}'
rancher/nginx-ingress-controller:v1.12.1-hardened1
----

. Run `kubectl -n kube-system edit helmchartconfig rke2-ingress-nginx` to *remove* the following configurations from the `HelmChartConfig` resource.
+
* `.spec.valuesContent.controller.admissionWebhooks.enabled: false`
* `.spec.valuesContent.controller.extraArgs.enable-annotation-validation: true`

. Verify that the new `.spec.ValuesContent` configuration is similar to the following example.
+
[,yaml]
----
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      admissionWebhooks:
        port: 8444
      extraArgs:
        default-ssl-certificate: cattle-system/tls-rancher-internal
      config:
        proxy-body-size: "0"
        proxy-request-buffering: "off"
      publishService:
        pathOverride: kube-system/ingress-expose
----
+
[IMPORTANT]
====
If the `HelmChartConfig` resource contains other custom `ingress-nginx` configuration, you must retain them when editing the resource.
====

. Exit the `kubectl edit` command execution to save the configuration.
+
{harvester-product-name} automatically applies the change once the content is saved.

. Verify that the `rke2-ingress-nginx-admission` webhook configuration is re-enabled.
+
[,shell]
----
$ kubectl get validatingwebhookconfiguration rke2-ingress-nginx-admission
NAME                           WEBHOOKS   AGE
rke2-ingress-nginx-admission   1          6s
----

. Verify that the `ingress-nginx` pods are restarted successfully.
+
[,shell]
----
kubectl -n kube-system get po -lapp.kubernetes.io/instance=rke2-ingress-nginx
NAME                                  READY   STATUS    RESTARTS   AGE
rke2-ingress-nginx-controller-l2cxz   1/1     Running   0          94s
----