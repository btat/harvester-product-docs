= Resource Quotas
:revdate: 2025-06-18
:page-revdate: {revdate}

https://kubernetes.io/docs/concepts/policy/resource-quotas/[ResourceQuota] is used to limit the usage of resources within a namespace. It helps administrators control and restrict the allocation of cluster resources to ensure fairness and controlled resource distribution among namespaces.

In {harvester-product-name}, ResourceQuota can define usage limits for the following resources:

* *CPU*: Limits compute resource usage, including CPU cores and CPU time.
* *Memory*: Limits the usage of memory resources in bytes or other recognizable memory units.
* *Storage*: Limits the usage of storage resources.

== Set ResourceQuota via Rancher

Administrators can configure resource quotas for namespaces by performing the following steps:

. On the {rancher-short-name} UI, go to *â‰¡ -> Virtualization Management*.

. Select a cluster, go to *Projects/Namespaces*, and then click *Create Project*.

. In the general information section, specify a name and description for the project.

. On the *Resource Quotas* tab, click *Add Resource*, select a resource type, and specify the corresponding limit values.
+
image::rancher/create-project.png[]

. Click *Create*.

[NOTE]
====
The *VM Default Resource Limit* tab contains resource reservation and limit settings that are applied only to pod workloads running within the namespace. Values configured on this tab correspond to the `defaultRequest` and `default` values of the namespace's https://kubernetes.io/docs/concepts/policy/limit-range/[`LimitRange`] policy. These settings may be removed in a future release.
====

You can configure the *Namespace* limits as follows:

. Find the newly created project, and select *Create Namespace*.
. Specify the desired namespace *Name*, and adjust the limits.
. Complete the process by selecting *Create*.
+
image::rancher/create-namespace.png[]

[NOTE]
====
Attempts to provision virtual machines for guest clusters are blocked when the resource quotas are reached. {rancher-short-name} responds by creating a new virtual machine in a loop, in which each failed creation attempt is immediately followed by another attempt. This results in a transient error state in the cluster that is not recorded as the virtual machine is recreated.
====

== Overhead memory of virtual machine

Upon creating a virtual machine (VM), the VM controller seamlessly incorporates overhead resources into the VM's configuration. These additional resources intend to guarantee the consistent and uninterrupted functioning of the VM. It's important to note that configuring memory limits requires a higher memory reservation due to the inclusion of these overhead resources.

For example, consider the creation of a new VM with the following configuration:

* CPU: 8 cores
* Memory: 16Gi

[NOTE]
====
The operating system, either Linux or Windows, does not affect overhead calculations.
====

Memory Overhead is calculated in the following sections:

* *Memory PageTables Overhead:* This accounts for one bit for every 512b RAM size. For instance, a memory of 16Gi requires an overhead of 32Mi.
* *VM Fixed Overhead:* This consists of several components:
 ** `VirtLauncherMonitorOverhead`: 25Mi  (the `ps` RSS for virt-launcher-monitor)
 ** `VirtLauncherOverhead`: 75Mi  (the `ps` RSS for the virt-launcher process)
 ** `VirtlogdOverhead`: 17Mi  (the `ps` RSS for virtlogd)
 ** `LibvirtdOverhead`: 33Mi (the `ps` RSS for libvirtd)
 ** `QemuOverhead` : 30Mi (the `ps` RSS for qemu, minus the RAM of its (stressed) guest, minus the virtual page table)
* *8Mi per CPU (vCPU) Overhead:* Additionally, 8Mi of overhead per vCPU is added, along with a fixed 8Mi overhead for IOThread.
* *Extra Added Overhead:* This encompasses various factors like video RAM overhead and architecture overhead. Refer to https://github.com/kubevirt/kubevirt/blob/2bb88c3d35d33177ea16c0f1e9fffdef1fd350c6/pkg/virt-controller/services/template.go#L1853-L1890[Additional Overhead] for further details.

This calculation demonstrates that the VM instance necessitates an additional memory overhead of approximately 276Mi.

For more information, see https://kubevirt.io/user-guide/virtual_machines/virtual_hardware/#memory-overhead[Memory Overhead].

For more information on how the memory overhead is calculated in Kubevirt, refer to https://github.com/kubevirt/kubevirt/blob/v0.54.0/pkg/virt-controller/services/template.go#L1804[kubevirt/pkg/virt-controller/services/template.go].

== Automatic adjustment of ResourceQuota during migration

When the allocated resource quota controlled by the `ResourceQuota` object reaches its limit, migrating a VM becomes unfeasible. The migration process automatically creates a new pod mirroring the resource requirements of the source VM. If these pod creation prerequisites surpass the defined quota, the migration operation cannot proceed.

In {harvester-product-name}, the `ResourceQuota` values will dynamically expand ahead of migration to accommodate the resource needs of the target virtual machine. After migration, the ResourceQuotas will be reinstated to their prior configurations.

Please be aware of the following constrains of the automatic resizing of `ResourceQuota`:

* `ResourceQuota` cannot be changed during VM migration.
* When raising the `ResourceQuota` value, if you create, start, or restore other VMs, {harvester-product-name} will verify if the resources are sufficient based on the original `ResourceQuota`. If the conditions are not met, the system will alert that the migration process is not feasible.
* After expanding `ResourceQuota`, potential resource contention may occur between non-VM pods and VM pods, leading to migration failures. Therefore, deploying custom container workloads and VMs to the same namespace is not recommended.
* Due to the concurrent limitation of the webhook validator, the VM controller will execute a secondary validation to confirm resource sufficiency. If the resource is insufficient, it will auto config the VM's `RunStrategy` to `Halted`, and a new annotation `harvesterhci.io/insufficient-resource-quota` will be added to the VM object, informing you that the VM was shut down due to insufficient resources.
+
image::rancher/vm-annotation-insufficient-resource-quota.png[]

=== Disable Automatic Adjustment of ResourceQuota During Migration

When a `ResourceQuota` object has the annotation `harvesterhci.io/skipResourceQuotaAutoScaling: "true"`, {harvester-product-name} does not automatically adjust the values of that object. This feature is useful for debugging, troubleshooting and other tasks.

[IMPORTANT]
====
You must set the annotation before the migration starts. If the annotation is set while the values are already being adjusted, {harvester-product-name} is unable to automatically restore the previous configuration.
====