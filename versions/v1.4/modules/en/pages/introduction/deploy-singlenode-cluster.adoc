= Deploy a Single-Node Cluster
:revdate: 2024-11-21
:page-revdate: {revdate}

A {harvester-product-name} cluster with three or more nodes is required to fully realize multi-node features such as high availability. The latest versions allow you to create clusters with two management nodes and one xref:../hosts/witness-node.adoc[witness node] (and optionally, one or more worker nodes). You can also create xref:../installation-setup/single-node-clusters.adoc[single-node clusters] that support most features (excluding high availability, multi-replica support, and live migration).

This guide walks you through the steps required to deploy a *single-node cluster* and virtual machines (VMs) that can host guest clusters and run custom workloads.

== 1. Verify that the minimum hardware and network requirements are met.

{harvester-product-name} is built for bare metal servers using enterprise-grade open-source software components. The installer automatically checks the hardware and displays warning messages if the minimum xref:../installation-setup/requirements.adoc[requirements] are not met.

== 2. Prepare the installation files based on the installation method that you want to use.

You can download the installation files from the https://github.com/harvester/harvester/releases[Releases] page. The *Downloads* section of the release notes contains links to the ISO files and related artifacts. The following types of ISO files are available:

* *Full ISO*: Contains the core operating system components and all required container images, which are preloaded during installation. You must use a full ISO when installing {harvester-product-name} behind a firewall or proxy, and in environments without internet connectivity.
* xref:../installation-setup/media/net-install.adoc[*Net install ISO*]: Contains only the core operating system components. After installation is completed, the operating system pulls all required container images from the internet (mostly from Docker Hub).

|===
| Method | Required Installation Files | Other Requirements

| xref:../installation-setup/methods/iso-install.adoc[ISO]
| ISO
| N/A

| xref:../installation-setup/methods/usb-install.adoc[USB]
| ISO
| USB flash drive; utility such as https://etcher.balena.io/[balenaEtcher] or the Linux https://man7.org/linux/man-pages/man1/dd.1.html[dd command]

| xref:../installation-setup/methods/pxe-boot-install.adoc[PXE]
| ISO, Linux kernel image (vmlinuz), initrd, SquashFS image
| Directory on the HTTP server (for serving boot files); iPXE boot scripts (for automatic installation); DHCP server configuration
|===

== 3. Prepare the cluster configuration requirements.

* Cluster token: ASCII string that nodes use when joining the cluster
* Fixed IP address for each node: May be assigned statically or using DHCP (host reservation)
* Fixed virtual IP address (VIP) to be used as the cluster management address: VIP that you connect to when performing administration tasks after the cluster is deployed
* Addresses of DNS servers, NTP servers, and the proxy server (if necessary)

== 4. Deploy the cluster node.

Deployment involves installing the operating system and other components on the host, and then rebooting once installation is completed. Deploying the node creates the cluster, and the node is assigned the management role by default.

During installation, you must configure node settings, define the cluster management address (VIP) and the cluster token, and specify other information. If necessary, you can configure more settings using a xref:../installation-setup/config/configuration-file.adoc[configuration file].

Once installation is completed, the node restarts and then the console appears. The console displays information about the cluster (management URL and status) and the node (hostname, IP address, and status). After the cluster is initialized and all services start running, the cluster status changes to *Ready*.

== 5. Configure a strong password for the default `admin` user.

Once the cluster status changes to *Ready*, you can access the xref:../installation-setup/authentication.adoc[UI] using the management URL displayed on the console.

== 6. Configure the default StorageClass.

{harvester-product-name} uses StorageClasses to describe how Longhorn must provision volumes. Each StorageClass has a parameter that defines the number of replicas to be created for each volume.

The default StorageClass `harvester-longhorn` has a replica count value of *3* for high availability. If you use `harvester-longhorn` in your single-node cluster, Longhorn is unable to create the default number of replicas, and volumes are marked as _Degraded_ on the UI.

To avoid this issue, you can perform either of the following actions:

* Change the xref:../installation-setup/config/configuration-file.adoc#_install_harvester_storage_class_replica_count[replica count] of `harvester-longhorn` to *1* using a xref:../installation-setup/config/configuration-file.adoc[configuration file].
* xref:../storage/storageclass.adoc#_creating_a_storageclass[Create a new StorageClass] with the *Number of Replicas* parameter set to *1*. Once created, locate the new StorageClass in the list and then select *â‹®* > *Set as Default*.

== 7. Create a custom cluster network and a VM network. (Optional)

Networking involves three major concepts:

* xref:../networking/cluster-network.adoc#_cluster_network[*Cluster network*]: Traffic-isolated forwarding path for transmission of network traffic in the {harvester-product-name} cluster.
+
During deployment, a cluster network called xref:../networking/cluster-network.adoc#_built_in_cluster_network[`mgmt`] is created for intra-cluster communications. `mgmt` allows VMs to be accessed from the infrastructure network (external to the cluster) to which each node attaches with management NICs for cluster management purposes. {harvester-product-name} also allows you to create xref:../networking/cluster-network.adoc#_custom_cluster_network[custom cluster networks] that can be dedicated to VM traffic.

* xref:../networking/cluster-network.adoc#_network_configuration[*Network configuration*]: Definition of how cluster nodes connect to a specific cluster network.
+
Each network configuration corresponds to a set of nodes with uniform network specifications. Only nodes that are covered by the network configuration can access the associated cluster network. This arrangement offers you flexibility when configuring a heterogeneous cluster, particularly when the network interface names are different for each node.

* xref:../networking/cluster-network.adoc#_vm_network[*VM network*]: Virtual network that VMs use to communicate with other VMs and external networks.
+
Each VM network is linked to a specific cluster network, which is used for transmission of VM traffic. You can create either a xref:../networking/vm-network.adoc#_vlan_network[VLAN network] or an xref:../networking/vm-network.adoc#_untagged_network[untagged network] based on your requirements, such as traffic isolation, network segmentation, ease of management, or alignment with the external network environment.

You can create a VM network that uses `mgmt` when testing {harvester-product-name} with a single-node cluster.

== 8. Import VM images.

On the UI, you can import ISO, qcow2, and raw xref:../virtual-machines/vm-images/upload-image.adoc[images] by uploading an image from the local file system, or by specifying the URL of an image that can be accessed from the cluster.

== 9. Import SSH keys. (Recommended)

You can store SSH public keys in {harvester-product-name}. When a VM is launched, a stored key can be xref:../virtual-machines/access-vm.adoc#_ssh_access[injected] into the VM to allow secure access via SSH. Validated keys are displayed on the *SSH Keys* screen on the UI.

== 10. Create VMs.

You can create xref:../virtual-machines/create-vm.adoc[Linux VMs] using one of the following methods:

* UI: On the *Virtual Machines* screen, click *Create* and configure the settings on each tab.
* Kubernetes API: Create a `VirtualMachine` object.
* xref:../integrations/terraform/terraform-provider.adoc[Terraform Provider]: Define a `harvester_virtualmachine` resource block.

Creating xref:../virtual-machines/create-windows-vm.adoc[Windows VMs] on the UI involves slightly different steps. {harvester-product-name} provides a VM template named `windows-iso-image-base-template` that adds a volume with the Virtio drivers for Windows, which streamlines the VM configuration process. If you require Virtio devices but choose to not use the template, you must add your own Virtio drivers for Windows to enable correct hardware detection.

== What's Next

The following sections provide guides that walk you through how to back up and restore VMs, manage hosts, and use {rancher-product-name} with {harvester-product-name}.

* xref:../virtual-machines/backup-restore.adoc[VM Backup, Snapshot & Restore]
* xref:../hosts/hosts.adoc[Host Management]
* xref:../integrations/rancher/rancher-integration.adoc[{rancher-product-name} Integration]
