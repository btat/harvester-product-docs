= Creating an RKE2 Kubernetes Cluster
:revdate: 2025-06-09
:page-revdate: {revdate}

You can now provision {rke2-product-name} Kubernetes clusters on top of the {harvester-product-name} cluster in {rancher-product-name} using the built-in Harvester Node Driver.

image::rancher/rke2-k3s-node-driver.png[rke2-cluster]

[NOTE]
====

* xref:../../../networking/vm-network.adoc#_vlan_network[VLAN network] is required for Harvester Node Driver.
* Provisioning RKE2 Kubernetes clusters involves configuring the IP address of the underlying virtual machines. You can do this using a DHCP server on the VLAN network that the virtual machines are attached to. If such a server does not exist on the network, you can use the xref:../../../add-ons/vm-dhcp-controller.adoc[Managed DHCP] feature to configure the IP address.
* Harvester Node Driver only supports cloud images.
* For the port requirements of the guest clusters deployed within {harvester-product-name}, please refer to the doc xref:../../../installation-setup/requirements.adoc#_port_requirements_for_k3s_or_rkerke2_clusters[here].
* For RKE2 with Harvester Cloud Provider support matrix, please refer to the website https://www.suse.com/suse-harvester/support-matrix/all-supported-versions/[here].
====

== Backward Compatibility Notice

[NOTE]
====
Please note a known backward compatibility issue if you're using the Harvester Cloud Provider version *v0.2.2* or higher.  If your {harvester-product-name} version is below *v1.2.0* and you intend to use newer RKE2 versions (i.e., >= `v1.26.6+rke2r1`, `v1.25.11+rke2r1`, `v1.24.15+rke2r1`), it is essential to upgrade your {harvester-product-name} cluster to v1.2.0 or a higher version before proceeding with the upgrade of the guest Kubernetes cluster or Harvester Cloud Provider.

For a detailed support matrix, please refer to the *Harvester CCM & CSI Driver with RKE2 Releases* section of the official https://www.suse.com/suse-harvester/support-matrix/all-supported-versions/[website].
====

== Create your cloud credentials

. Click *â˜° > Cluster Management*.
. Click *Cloud Credentials*.
. Click *Create*.
. Click *Harvester*.
. Enter your cloud credential name
. Select "Imported Harvester Cluster".
. Click *Create*.

image::rancher/create-cloud-credentials.png[create-harvester-cloud-credentials]

== Create RKE2 kubernetes cluster

Users can create a RKE2 Kubernetes cluster from the *Cluster Management* page via the RKE2 node driver.

. Select *Clusters* menu.
. Click *Create* button.
. Toggle Switch to *RKE2/K3s*.
. Select Harvester Node Driver.
. Select a *Cloud Credential*.
. Enter *Cluster Name* (required).
. Enter *Namespace* (required).
. Enter *Image* (required).
. Enter *Network Name* (required).
. Enter *SSH User* (required).
. (optional) Configure the menu:Show Advanced[User Data] to install the required packages of VM.
+
[,yaml]
----
#cloud-config
packages:
  - iptables
----
+
[NOTE]
====
Calico and Canal networks require the `iptables` or `xtables-nft` package to be installed on the node. For more information, see https://documentation.suse.com/cloudnative/rke2/latest/en/known_issues.html#_canal_and_ip_exhaustion[Canal and IP exhaustion] in the RKE2 documentation.
====

. Click *Create*.
+
image:rancher/create-rke2-harvester-cluster-1.png[create-rke2-harvester-cluster-1]
image:rancher/create-rke2-harvester-cluster-2.png[create-rke2-harvester-cluster-2]
image:rancher/create-rke2-harvester-cluster-3.png[create-rke2-harvester-cluster-3]
+
[NOTE]
====
* RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration.
* Only imported {harvester-product-name} clusters are supported by the Harvester Node Driver.
====

=== Add node affinity

The Harvester Node Driver now supports scheduling a group of machines to particular nodes through the node affinity rules, which can provide high availability and better resource utilization.

Node affinity can be added to the machine pools during the cluster creation:

. Click the `Show Advanced` button and click the `Add Node Selector`
image:rancher/affinity-rke2-add-node-selector.png[affinity-add-node-selector]
. Set priority to `Required` if you wish the scheduler to schedule the machines only when the rules are met.
. Click `Add Rule` to specify the node affinity rules, e.g., for the xref:./node-driver.adoc#_topology_spread_constraints[topology spread constraints] use case, you can add the `region` and `zone` labels as follows:
+
[,yaml]
----
key: topology.kubernetes.io/region
operator: in list
values: us-east-1
---
key: topology.kubernetes.io/zone
operator: in list
values: us-east-1a
----
+
image::rancher/affinity-rke2-add-rules.png[affinity-add-rules]

=== Add workload affinity

The workload affinity rules allow you to constrain which nodes your machines can be scheduled on based on the labels of workloads (VMs and Pods) already running on these nodes, instead of the node labels.

Workload affinity rules can be added to the machine pools during the cluster creation:

. Select *Show Advanced* and choose *Add Workload Selector*.
image:rancher/affinity-rke2-add-workload-selector.png[affinity-add-workload-selector]
. Select *Type*, *Affinity* or *Anti-Affinity*.
. Select *Priority*. *Prefered* means it's an optional rule, and *Required* means a mandatory rule.
. Select the namespaces for the target workloads.
. Select *Add Rule* to specify the workload affinity rules.
. Set *Topology Key* to specify the label key that divides {harvester-product-name} hosts into different topologies.

See the https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity[Kubernetes Pod Affinity and Anti-Affinity Documentation] for more details.

== Update RKE2 Kubernetes cluster

The fields highlighted below of the RKE2 machine pool represent the {harvester-product-name} VM configurations. Any modifications to these fields will trigger node reprovisioning.

image::rancher/rke2-harvester-fields.png[rke2-harvester-fields]

== Using Harvester RKE2 node driver in air gapped environment

RKE2 provisioning relies on the `qemu-guest-agent` package to get the IP of the virtual machine.

Calico and Canal require the `iptables` or `xtables-nft` package to be installed on the node.

However, it may not be feasible to install packages in an air gapped environment.

You can address the installation constraints with the following options:

* Option 1. Use a VM image preconfigured with required packages (e.g., `iptables`, `qemu-guest-agent`).
* Option 2. Go to *Show Advanced* > *User Data* to allow VMs to install the required packages via an HTTP(S) proxy.

Example user data in {harvester-product-name} node template:

----
#cloud-config
apt:
  http_proxy: http://192.168.0.1:3128
  https_proxy: http://192.168.0.1:3128
----