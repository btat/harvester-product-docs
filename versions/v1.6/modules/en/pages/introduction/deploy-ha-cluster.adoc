= Deploy a High-Availability Cluster
:revdate: 2025-06-10
:page-revdate: {revdate}

A {harvester-product-name} cluster with three or more nodes is required to fully realize multi-node features such as high availability. The latest versions allow you to create clusters with two management nodes and one xref:/hosts/witness-node.adoc[witness node] (and optionally, one or more worker nodes). You can also create xref:/installation-setup/single-node-clusters.adoc[single-node clusters] that support most features (excluding high availability, multi-replica support, and live migration).

This guide walks you through the steps required to deploy a *high-availability cluster* and virtual machines (VMs) that can host guest clusters and run custom workloads.

== 1. Verify that the minimum hardware and network requirements are met.

{harvester-product-name} is built for bare metal servers using enterprise-grade open-source software components. The installer automatically checks the hardware and displays warning messages if the minimum xref:/installation-setup/requirements.adoc[requirements] are not met.

== 2. Prepare the installation files based on the installation method that you want to use.

You can download the installation files from the https://github.com/harvester/harvester/releases[Releases] page. The *Downloads* section of the release notes contains links to the ISO files and related artifacts. The following types of ISO files are available:

* *Full ISO*: Contains the core operating system components and all required container images, which are preloaded during installation. You must use a full ISO when installing {harvester-product-name} behind a firewall or proxy, and in environments without internet connectivity.
* xref:/installation-setup/media/net-install.adoc[*Net install ISO*]: Contains only the core operating system components. After installation is completed, the operating system pulls all required container images from the internet (mostly from Docker Hub).

|===
| Method | Required Installation Files | Other Requirements

| xref:/installation-setup/methods/iso-install.adoc[ISO]
| ISO
| N/A

| xref:/installation-setup/methods/usb-install.adoc[USB]
| ISO
| USB flash drive; utility such as https://etcher.balena.io/[balenaEtcher] or the Linux https://man7.org/linux/man-pages/man1/dd.1.html[dd command]

| xref:/installation-setup/methods/pxe-boot-install.adoc[PXE]
| ISO, Linux kernel image (vmlinuz), initrd, SquashFS image
| Directory on the HTTP server (for serving boot files); iPXE boot scripts (for automatic installation); DHCP server configuration
|===

== 3. Prepare the cluster configuration requirements.

* Cluster token: ASCII string that nodes use when joining the cluster
* Fixed IP address for each node: May be assigned statically or using DHCP (host reservation)
* Fixed virtual IP address (VIP) to be used as the cluster management address: VIP that nodes point to when joining the cluster and that you connect to when performing administration tasks after the cluster is deployed
* Addresses of DNS servers, NTP servers, and the proxy server (if necessary)

== 4. Deploy the first cluster node.

Deployment involves installing the operating system and other components on the host, and then rebooting once installation is completed. Deploying the first node creates the cluster, and the first node is assigned the management node by default.

During installation, you must configure node settings, define the cluster management address (VIP) and the cluster token, and specify other information. If necessary, you can configure more settings using a xref:/installation-setup/config/configuration-file.adoc[configuration file].

Once installation is completed, the node restarts and then the console appears. The console displays information about the cluster (management URL and status) and the node (hostname, IP address, and status). After the cluster is initialized and all services start running, the cluster status changes to *Ready*.

== 5. Configure a strong password for the default `admin` user.

Once the cluster status changes to *Ready*, you can access the xref:/installation-setup/authentication.adoc[UI] using the management URL displayed on the console.

== 6. Deploy the other nodes and join them to the cluster.

Deployment involves installing the operating system and other components on the host, and then rebooting once installation is completed. All other nodes join the cluster that was created when the first node was deployed.

During installation, you must configure node settings, and specify the cluster management address (virtual IP) and the cluster token that you defined previously. If necessary, you can configure more settings using a xref:/installation-setup/config/configuration-file.adoc[configuration file].

When the cluster has three or more nodes, the two nodes added after the first node are automatically promoted to management nodes to form a high-availability (HA) cluster.

== 7. Create a custom cluster network (optional) and a VM network (required).

Networking involves three major concepts:

* xref:/networking/cluster-network.adoc#_cluster_network[*Cluster network*]: Traffic-isolated forwarding path for transmission of network traffic in the {harvester-product-name} cluster.
+
During deployment, a cluster network called xref:/networking/cluster-network.adoc#_built_in_cluster_network[`mgmt`] is created for intra-cluster communications. `mgmt` allows VMs to be accessed from the infrastructure network (external to the cluster) to which each node attaches with management NICs for cluster management purposes. {harvester-product-name} also allows you to create xref:/networking/cluster-network.adoc#_custom_cluster_network[custom cluster networks] that can be dedicated to VM traffic.

* xref:/networking/cluster-network.adoc#_network_configuration[*Network configuration*]: Definition of how cluster nodes connect to a specific cluster network.
+
Each network configuration corresponds to a set of nodes with uniform network specifications. Only nodes that are covered by the network configuration can access the associated cluster network. This arrangement offers you flexibility when configuring a heterogeneous cluster, particularly when the network interface names are different for each node.

* xref:/networking/cluster-network.adoc#_vm_network[*VM network*]: Virtual network that VMs use to communicate with other VMs and the external infrastructure network.
+
Each VM network is linked to a specific cluster network, which is used for transmission of VM traffic. You can create either a xref:/networking/vm-network.adoc#_vlan_network[VLAN network] or an xref:/networking/vm-network.adoc#_untagged_network[untagged network] based on your requirements, such as traffic isolation, network segmentation, ease of management, or alignment with the external network environment.

Before you create VMs, create the necessary networks. If more than one network interface is attached to each cluster node, consider xref:/networking/cluster-network.adoc#_create_a_new_cluster_network[creating custom cluster networks] and network configurations for better traffic isolation. Otherwise, you can only use the management network for transmission of VM traffic. Next, xref:/networking/vm-network.adoc#_create_a_vm_network[create a VM network] that is linked to either `mgmt` or any of the custom cluster networks that you created.

== 8. Import VM images.

On the UI, you can import ISO, qcow2, and raw xref:/virtual-machines/vm-images/upload-image.adoc[images] by uploading an image from the local file system, or by specifying the URL of an image that can be accessed from the cluster.

== 9. Import SSH keys. (Recommended)

You can store SSH public keys in {harvester-product-name}. When a VM is launched, a stored key can be xref:/virtual-machines/access-vm.adoc#_ssh_access[injected] into the VM to allow secure access via SSH. Validated keys are displayed on the *SSH Keys* screen on the UI.

== 10. Create VMs.

You can create xref:/virtual-machines/create-vm.adoc[Linux VMs] using one of the following methods:

* UI: On the *Virtual Machines* screen, click *Create* and configure the settings on each tab.
* Kubernetes API: Create a `VirtualMachine` object.
* xref:/integrations/terraform/terraform-provider.adoc[Terraform Provider]: Define a `harvester_virtualmachine` resource block.

Creating xref:/virtual-machines/create-windows-vm.adoc[Windows VMs] on the UI involves slightly different steps. {harvester-product-name} provides a VM template named `windows-iso-image-base-template` that adds a volume with the Virtio drivers for Windows, which streamlines the VM configuration process. If you require Virtio devices but choose to not use the template, you must add your own Virtio drivers for Windows to enable correct hardware detection.

== What's Next

The following sections provide guides that walk you through how to back up and restore VMs, manage hosts, and use {rancher-product-name} with {harvester-product-name}.

* xref:/virtual-machines/backup-restore.adoc[VM Backup, Snapshot & Restore]
* xref:/hosts/hosts.adoc[Host Management]
* xref:/integrations/rancher/rancher-integration.adoc[{rancher-product-name} Integration]
