= Upgrade from v1.4.1 to v1.4.2

== General information

An *Upgrade* button appears on the *Dashboard* screen whenever a new {harvester-product-name} version that you can upgrade to becomes available. For more information, see xref:./upgrades.adoc#_start_an_upgrade[Start an upgrade].

For air-gapped environments, see xref:./upgrades.adoc#_prepare_an_air_gapped_upgrade[Prepare an air-gapped upgrade].

=== Update Harvester UI Extension on {rancher-product-name} v2.10.1

To import {harvester-product-name} v1.4.2 clusters on {rancher-short-name} v2.10.1, you must use **v1.0.3** of the {rancher-short-name} UI extension for {harvester-product-name}.

. On the {rancher-short-name} UI, go to *local -> Apps -> Repositories*.

. Locate the repository named *harvester*, and then select *â‹® -> Refresh*.
+
This repository has the following properties:
+
* URL: **https://github.com/harvester/harvester-ui-extension**
* Branch: **gh-pages**
+
image::upgrade/rancher-2.10.1-repository-page.png[]

. Go to the *Extensions* screen.

. Locate the extension named *Harvester*, and then click *Update*.

. Select version *1.0.3*, and then click *Update*.
+
image::upgrade/update-harvester-ui-extension-modal.png[]

. Allow some time for the extension to be updated, and then refresh the screen.

[IMPORTANT]
====
The {rancher-short-name} UI displays an error message after the extension is updated. The error message disappears when you refresh the screen.

This issue, which exists in {rancher-short-name} v2.10.0 and v2.10.1, will be fixed in v2.10.2. 
====

Related issues: https://github.com/harvester/harvester/issues/7234[#7234], https://github.com/rancher/capi-ui-extension/issues/107[#107]

== Virtual machine backup compatibility

You may encounter certain limitations when creating and restoring backups that involve external storage.

== Known issues

=== 1. Upgrade stuck in "Pre-drained" state

The upgrade process may become stuck in the "Pre-drained" state. Kubernetes is supposed to drain the workload on the node, but some factors may cause the process to stall.

image::upgrade/3730-stuck.png[]

A possible cause is processes related to orphan engines of the Longhorn Instance Manager. To determine if this applies to your situation, perform the following steps:

. Check the name of the `instance-manager` pod on the stuck node.
+
Example:
+
The stuck node is `harvester-node-1`, and the name of the Instance Manager pod is `instance-manager-d80e13f520e7b952f4b7593fc1883e2a`.
+
[,shell]
----
$ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager
instance-manager-d80e13f520e7b952f4b7593fc1883e2a          1/1     Running   0              3d8h
----

. Check the Longhorn Manager logs for informational messages.
+
Example:
+
[,shell]
----
$ kubectl -n longhorn-system logs daemonsets/longhorn-manager
...
time="2025-01-14T00:00:01Z" level=info msg="Node instance-manager-d80e13f520e7b952f4b7593fc1883e2a is marked unschedulable but removing harvester-node-1 PDB is blocked: some volumes are still attached InstanceEngines count 1 pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0" func="controller.(*InstanceManagerController).syncInstanceManagerPDB" file="instance_manager_controller.go:823" controller=longhorn-instance-manager node=harvester-node-1
----
+
The `instance-manager` pod cannot be drained because of the engine `pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0`.

. Check if the engine is still running on the stuck node.
+
Example:
+
[,shell]
----
$ kubectl -n longhorn-system get engines.longhorn.io pvc-9ae0e9a5-a630-4f0c-98cc-b14893c74f9e-e-0 -o jsonpath='{"Current state: "}{.status.currentState}{"\nNode ID: "}{.spec.nodeID}{"\n"}'
Current state: stopped
Node ID:
----
+
The issue likely exists if the output shows that the engine is either not running or not found.

. Check if all volumes are healthy.
+
[,shell]
----
kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == "attached")| .status.robustness'
----
+
All volumes must be marked `healthy`. If this is not the case, report the issue.

. Remove the `instance-manager` pod's PodDisruptionBudget (PDB).
+
Example:
+
[,shell]
----
kubectl delete pdb instance-manager-d80e13f520e7b952f4b7593fc1883e2a -n longhorn-system
----

Related issues: https://github.com/harvester/harvester/issues/7366[#7366] and https://github.com/longhorn/longhorn/issues/6764[#6764]

=== 2. High CPU usage

High CPU usage may occur because of the `backup-target` setting's `refreshIntervalInSeconds` field. If the field is left empty or is set to `0`, {harvester-product-name} constantly refreshes the backup target, resulting in high CPU usage.

To fix the issue, update the value of `refreshIntervalInSeconds` to a larger number (for example, `60`) using the command `kubectl edit setting backup-target`.

Example:

[,shell]
----
value: '{"type":"nfs","endpoint":"nfs://longhorn-test-nfs-svc.default:/opt/backupstore", "refreshIntervalInSeconds": 60}'
----

Related issue: https://github.com/harvester/harvester/issues/7885[#7885]

=== 3. Upgrade restarts unexpectedly after the "Dismiss it" button is clicked

When you use {rancher-short-name} to upgrade {harvester-product-name}, the {rancher-short-name} UI displays a dialog with a button labeled "Dismiss it". Clicking this button may result in the following issues:

* The `status` section of the `harvesterhci.io/v1beta1/upgrade` CR is cleared, causing the loss of all important information about the upgrade.
* The upgrade process restarts unexpectedly.

This issue affects {rancher-short-name} v2.10.x, which uses v1.0.2, v1.0.3, and v1.0.4 of the xref:../integrations/rancher/harvester-ui-extension.adoc#_support_matrix[Harvester UI Extension]. All {harvester-product-name} UI versions are not affected. The issue is fixed in Harvester UI Extension v1.0.5 and v1.5.0.

To avoid this issue, perform either of the following actions:

* Use the {harvester-product-name} UI for upgrades. Clicking the "Dismiss it" button on the {harvester-product-name} UI does not result in unexpected behavior.
* Instead of clicking the button on the {rancher-short-name} UI, run the following command against the cluster:
+
[,shell]
----
kubectl -n harvester-system label upgrades -l harvesterhci.io/latestUpgrade=true harvesterhci.io/read-message=true
----

Related issue: https://github.com/harvester/harvester/issues/7791[#7791]

=== 4. Virtual machines that use migratable RWX volumes restart unexpectedly

Virtual machines that use migratable xref:integrations/rancher/csi-driver.adoc#_rwx_volumes_support[RWX volumes] restart unexpectedly when the CSI plugin pods are restarted. This issue affects {harvester-product-name} v1.4.x, v1.5.0, and v1.5.1.

The workaround is to disable the setting https://documentation.suse.com/cloudnative/storage/1.8/en/longhorn-system/settings.html#_automatically_delete_workload_pod_when_the_volume_is_detached_unexpectedly[Automatically Delete Workload Pod When The Volume Is Detached Unexpectedly] on the {longhorn-product-name} UI before starting the upgrade. You must enable the setting again once the upgrade is completed.

The issue will be fixed in {longhorn-product-name} v1.8.3, v1.9.1, and later versions. {harvester-product-name} v1.6.0 will include {longhorn-product-name} v1.9.1. 

Related issues: https://github.com/harvester/harvester/issues/8534[#8534] and https://github.com/longhorn/longhorn/issues/11158[#11158]