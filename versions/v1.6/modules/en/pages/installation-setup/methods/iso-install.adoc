= ISO Installation
:revdate: 2025-10-20
:page-revdate: {revdate}

{harvester-product-name} ships as a bootable appliance image, you can install it directly on a bare metal server with the ISO image. To get the ISO image, download *ðŸ’¿ harvester-v1.x.x-amd64.iso* from the https://github.com/harvester/harvester/releases[Releases] page.

During the installation, you can either choose to *create a new cluster* or *join the node to an existing cluster*.

The following https://youtu.be/X0VIGZ_lExQ[video] shows a quick overview of an ISO installation.

+++<div class="text-center">++++++<iframe width="800" height="400" src="https://www.youtube.com/embed/X0VIGZ_lExQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">++++++</iframe>++++++</div>+++

== Installation Steps

. Mount the ISO file and boot the server by selecting the `Harvester Installer` option.
+
image::install/iso-install.png[iso-install.png]
+
The installer automatically checks the hardware and displays warning messages if the minimum requirements are not met. The *Hardware Checks* screen is not displayed if all checks are passed.
+
image::install/hardware-checks.png[hardware-checks.png]

. Use the arrow keys to choose an installation mode. By default, the first node will be the management node of the cluster.
+
image::install/choose-installation-mode.png[choose-installation-mode.png]

 ** `Create a new Harvester cluster`: creates an entirely new cluster.
 ** `Join an existing Harvester cluster`: joins an existing cluster. You need the VIP and cluster token of the cluster you want to join.
 ** `Install Harvester binaries only`: If you choose this option, additional setup is required after the first bootup.

+

[IMPORTANT]
====
When there are 3 nodes, the other 2 nodes added first are automatically promoted to management nodes to form an HA cluster. If you want to promote management nodes from different zones, you can add the node label `topology.kubernetes.io/zone` in the xref:../../installation-setup/config/configuration-file.adoc#_os_labels[os.labels] config by providing a URL of xref:../../installation-setup/config/configuration-file.adoc[configuration file] on the customize the host step. In this case, at least three different zones are required.
====

. Choose a role for the node. You are required to perform this step if you selected the installation mode `Join an existing Harvester cluster`.
+
image::install/select-role.png[choose-node-role.png]

 ** `Default Role`: Allows a node to function as a management node or a worker node. This role does not have any specific privileges or restrictions.
 ** `Management Role`: Allows a node to be prioritized when {harvester-product-name} promotes nodes to management nodes.
 ** `Witness Role`: Restricts a node to being a witness node (only functions as an etcd node) in a specific cluster.
 ** `Worker Role`: Restricts a node to being a worker node (never promoted to management node) in a specific cluster.

. Choose the installation disk you want to install the cluster on and the data disk you want to store VM data on. By default, {harvester-product-name} uses https://en.wikipedia.org/wiki/GUID_Partition_Table[GUID Partition Table (GPT)] partitioning schema for both UEFI and BIOS. If you use the BIOS boot, then you will have the option to select https://en.wikipedia.org/wiki/Master_boot_record[Master boot record (MBR)].
+
image::install/choose-installation-target-data-disk.png[choose-installation-target-data-disk.png]

 ** `Installation disk`: The disk to install the cluster on.
 ** `Data disk`: The disk to store VM data on. Choosing a separate disk to store VM data is recommended. This does not apply to witness nodes.
 ** `Persistent size`: If you only have one disk or use the same disk for both OS and VM data, you need to configure persistent partition size to store system packages and container images. The default and minimum persistent partition size is 150 GiB. You can specify a size like 200Gi or 153600Mi.

. Configure the `HostName` of the node.
+
image::install/config-hostname.png[config-hostname.png]

. Configure network interface(s) for the management network. By default, {harvester-product-name} creates a xref:../../installation-setup/requirements.adoc#_hardware_requirements[bonded NIC] named `mgmt-bo` for the xref:../../networking/cluster-network.adoc#_built_in_cluster_network[built-in management cluster network], and the IP address can be configured via DHCP or statically assigned.
+
image::install/config-network.png[config-network.png]
+

[NOTE]
====
It is not possible to change the node IP throughout the lifecycle of a cluster. If using DHCP, you must ensure the DHCP server always offers the same IP for the same node. If the node IP is changed, the related node cannot join the cluster and might even break the cluster.

In addition, you are required to add the _routers_ option (`option routers`) when configuring the DHCP server. This option is used to add the default route on the host. Without the default route, the node will fail to start.

For example:

----
Linux~ # ip route
default via 192.168.122.1 dev mgmt-br proto dhcp
----

For more information, see xref:./pxe-boot-install.adoc#_dhcp_server_configuration[DHCP Server Configuration].
====

. (Optional) Configure the CIDRs for the cluster pods and services.
+
If you want to use the default values, leave the fields blank.
+
image::install/config-cluster-cidrs.png[config-cluster-cidrs.png]
+
[IMPORTANT]
====
The CIDR values must not overlap and must be within the private IP address range of 10.0.0.0/8, 172.16.0.0/12, or 192.168.0.0/16.

The DNS service IP must be within the range defined by the *Service CIDR* field.
====
+
Example of a valid CIDR configuration:
+
* *Pod CIDR*: 172.16.0.0/16
* *Service CIDR*: 172.22.0.0/16
* *Cluster DNS IP*: 172.22.0.10

. (Optional) Configure the `DNS Servers`. Use commas as a delimiter to add more DNS servers. Leave it blank to use the default DNS server.
+
image::install/config-dns-server.png[config-dns-server.png]

. Configure the virtual IP (VIP) by selecting a `VIP Mode`. This VIP is used to access the cluster or for other nodes to join the cluster.
+
[NOTE]
====
For DHCP setup with static MAC-to-IP address mappings configured, enter the MAC address in the provided field to fetch the unique persistent virtual IP (VIP). Otherwise, leave it blank.
====
+
image::install/config-virtual-ip.png[config-virtual-ip.png]

. Configure the `Cluster token`. This token is used for adding other nodes to the cluster.
+
image::install/config-cluster-token.png[config-cluster-token.png]

. Configure and confirm a `Password` to access the node. The default SSH user is `rancher`.
+
image::install/config-password.png[config-password.png]

. Configure `NTP servers` to make sure all nodes' times are synchronized. This defaults to `0.suse.pool.ntp.org`. Use commas as a delimiter to add more NTP servers.
+
image::install/config-ntp-server.png[config-ntp-server.png]
+
[NOTE]
====
Using multiple NTP servers provides redundancy, better accuracy, fault tolerance, and improved performance. It ensures that time synchronization continues even if one server fails or gives incorrect data, and helps distribute the load across different servers.
====

. (Optional) If you need to use an HTTP proxy to access the outside world, enter the `Proxy address`. Otherwise, leave this blank.
+
image::install/config-proxy.png[config-proxy.png]

. (Optional) You can choose to import SSH keys by providing `HTTP URL`. For example, your GitHub public keys `+https://github.com/<username>.keys+` can be used.
+
image::install/import-ssh-keys.png[import-ssh-keys.png]

. (Optional) If you need to customize the host with a xref:../../installation-setup/config/configuration-file.adoc[configuration file], enter the `HTTP URL` here.
+
image::install/remote-config.png[remote-config.png]

. Review and confirm your installation options. After confirming the installation options, {harvester-product-name} will be installed to your host. The installation may take a few minutes to be complete.
+
image::install/confirm-install.png[confirm-install.png]

. Once the installation is complete, your node restarts. After the restart, the console displays the management URL and status. The default URL of the web interface is `+https://your-virtual-ip+`. You can use `F12` to switch from the console to the Shell and type `exit` to go back to the console.
+

[NOTE]
====
Choosing `Install Harvester binaries only` on the first page requires additional setup after the first bootup.
====

+
image::install/iso-installed.png[iso-installed.png]

. You will be prompted to set the password for the default `admin` user when logging in for the first time.
+
image::install/first-time-login.png[first-login.png]

////

[NOTE]
====
In some cases, if you are using an older VGA connector, you may encounter an `panic: invalid dimensions` error with ISO installation. See issue [#2937](https://github.com/harvester/harvester/issues/2937#issuecomment-1278545927) for a workaround.
====

////

== Known Issue

=== Installer may crash when using an older graphics card/monitor

In some cases, if you are using an older graphics card/monitor, you may encounter a `panic: invalid dimensions` error during ISO installation.

image::install/invalid-dimensions.png[invalid-dimensions.png]

We are working on this known issue and planning a fix for a future release. You can try to use another GRUB entry to force it to use the resolution of `1024x768` when booting up.

image::install/force-resolution.png[force-resolution.png]

If you are using a version earlier than v1.1.1, please try the following workaround:

. Boot up with the ISO, and press `E` to edit the first menu entry:
+
image::install/grub-menu.png[grub-menu.png]

. Append `vga=792` to the line started with `$linux`:
+
image::install/edit-menu-entry.png[edit-menu-entry.png]

. Press `Ctrl+X` or `F10` to boot up.