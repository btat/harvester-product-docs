= Settings
:revdate: 2025-09-16
:page-revdate: {revdate}

The following is a list of advanced settings that you can use. You can modify the `settings.harvesterhci.io` custom resource using both the UI and the `kubectl` command.

== General Settings

=== `additional-ca`

*Definition*: Additional trusted CA certificates that enable {harvester-product-name} to access external services.

[CAUTION]
====
Changing this setting might cause single-node clusters to temporarily become unavailable or inaccessible.
====

*Default value*: None

*Example*:

----
-----BEGIN CERTIFICATE-----
SOME-CA-CERTIFICATES
-----END CERTIFICATE-----
----

=== `auto-disk-provision-paths` [Experimental]

*Definition*: Setting that allows {harvester-product-name} to automatically add disks that match the specified glob pattern as VM storage.

This setting only adds formatted disks that are mounted to the system. When specifying multiple patterns, separate values using commas.

[CAUTION]
==== 
This setting is applied to *all nodes* in the cluster. All data in the storage devices *will be destroyed*.
====

*Default value*: None

*Example*:

The following example adds disks that match the glob pattern `/dev/sd*` or `/dev/hd*`:

----
/dev/sd*,/dev/hd*
----

=== `auto-rotate-rke2-certs`

*Definition*: Setting that allows you to automatically rotate certificates for {rke2-product-name} services. This setting is disabled by default.

Use the field `expiringInHours` to specify the validity period of each certificate (`1` to `8759` hours). The certificate is automatically replaced before the specified period ends.

For more information, see the *Certificate Rotation* section of the https://documentation.suse.com/cloudnative/rancher-manager/v2.11/en/cluster-admin/manage-clusters/rotate-certificates.html[{rancher-product-name}] and https://documentation.suse.com/cloudnative/rke2/latest/en/advanced.html#_certificate_rotation[{rke2-product-name}] documentation.

*Default value*: `{"enable":false,"expiringInHours":240}`

*Example*:

----
{"enable":true,"expiringInHours":48}
----

=== `backup-target`

*Definition*: Custom backup target used to store VM backups.

For more information, see the https://documentation.suse.com/cloudnative/storage/1.8/en/snapshots-backups/volume-snapshots-backups/configure-backup-target.html#_set_up_aws_s3_backupstore[{longhorn-product-name} documentation].

*Default value*: None

*Example*:

[,json]
----
{
  "type": "s3",
  "endpoint": "https://s3.endpoint.svc",
  "accessKeyId": "test-access-key-id",
  "secretAccessKey": "test-access-key",
  "bucketName": "test-bup",
  "bucketRegion": "us‑east‑2",
  "cert": "",
  "virtualHostedStyle": false
}
----

=== `cluster-registration-url`

*Definition*: URL used to import the {harvester-product-name} cluster into {rancher-product-name} for multi-cluster management.

When you configure this setting, a new pod called `cattle-cluster-agent-*` is created in the namespace `cattle-system` for registration purposes. This pod uses the container image `rancher/rancher-agent:related-version`, which is not packed into the {harvester-product-name} ISO and is instead determined by {rancher-product-name}. The `related-version` is usually the same as the {rancher-product-name} version. For example, when you register {harvester-product-name} to {rancher-product-name} v2.7.9, the image is `rancher/rancher-agent:v2.7.9`. For more information, see https://documentation.suse.com/cloudnative/rancher-manager/v2.11/en/installation-and-upgrade/other-installation-methods/air-gapped/publish-images.html#_1_find_the_required_assets_for_your_rancher_version[Find the required assets for your Rancher version].

Depending on your settings, the image is downloaded from either of the following locations:

* {harvester-product-name} containerd-registry: You can configure a xref:_containerd_registry[private registry for the cluster].
* Docker Hub (docker.io): This is the default option when you do not configure a private registry in {rancher-product-name}.

Alternatively, you can obtain a copy of the image and manually upload it to all nodes.

*Default value*: None

*Example*:

----
https://172.16.0.1/v3/import/w6tp7dgwjj549l88pr7xmxb4x6m54v5kcplvhbp9vv2wzqrrjhrc7c_c-m-zxbbbck9.yaml
----

=== `containerd-registry`

*Definition*: Configuration of a private registry created for the {harvester-product-name} cluster.

The value is stored in the `registries.yaml` file of each node (path: `/etc/rancher/rke2/registries.yaml`). For more information, see https://documentation.suse.com/cloudnative/rke2/latest/en/install/containerd_registry_configuration.html[Containerd Registry Configuration] in the {rke2-product-name} documentation.

For security purposes, {harvester-product-name} automatically removes the username and password configured for the private registry after those credentials are stored in the `registries.yaml` file.

*Example*:

image::advanced/containerd-registry.png[containerd-registry]

[,json]
----
{
  "Mirrors": {
    "docker.io": {
      "Endpoints": ["https://myregistry.local:5000"],
      "Rewrites": null
    }
  },
  "Configs": {
    "myregistry.local:5000": {
      "Auth": {
        "Username": "testuser",
        "Password": "testpassword"
      },
      "TLS": {
        "InsecureSkipVerify": false
      }
    }
  }
}
----

=== `csi-driver-config`

*Definition*: Configuration necessary for using third-party CSI drivers installed in the cluster.

You must configure the following information before using features related to backups and snapshots:

* Provisioner for the installed third-party CSI driver
* `volumeSnapshotClassName`: Name of the `VolumeSnapshotClass` used to create volume snapshots or VM snapshots.
* `backupVolumeSnapshotClassName`: Name of the `VolumeSnapshotClass` used to create VM backups.

*Default value*:

----
{
  "driver.longhorn.io": {
    "volumeSnapshotClassName": "longhorn-snapshot",
    "backupVolumeSnapshotClassName": "longhorn"
  }
}
----

=== `csi-online-expand-validation`

*Definition*: Setting that allows you to mark storage providers with confirmed support for online volume expansion as validated.

Depending on the underlying storage provider, you may need to take extra steps to use the online volume expansion feature.

* *{longhorn-product-name}*: {harvester-product-name} considers {longhorn-product-name} to have support for online volume expansion, even if differences exist between versions of the Longhorn Data Engine. Currently, the V1 Data Engine fully supports online volume expansion, while the V2 Data Engine does not support volume expansion at all (regardless of the volume's attachment state). The {harvester-product-name} webhook manages the differences between these versions.

* *Third-party storage*: {harvester-product-name} rejects online volume expansion requests for third-party storage by default. If you have confirmed that your storage provider supports online volume expansion, you can use this setting to mark that storage provider as validated and force {harvester-product-name} to allow associated online expansion requests.

*Default value*: `{"driver.longhorn.io":true}`

=== `default-vm-termination-grace-period-seconds`

*Definition*: Number of seconds {harvester-product-name} waits before forcibly shutting down a VM that was stopped using the UI.

{harvester-product-name} sends a graceful shutdown signal to any VM that is stopped using the UI. If the graceful shutdown process is not completed within the specified number of seconds, {harvester-product-name} forcibly shuts down the VM.

*Default value*: `120`

=== `http-proxy`

*Definition*: HTTP proxy used to access external services, including downloading of images and backup to S3 services.

[CAUTION]
====
Changing this setting might cause single-node clusters to temporarily become unavailable or inaccessible.
====

*Default value*: `{}`

*Supported options and values*:

* Proxy URL for HTTP requests: `"httpProxy": "http://<username>:<pswd>@<ip>:<port>"`
* Proxy URL for HTTPS requests: `"httpsProxy": "https://<username>:<pswd>@<ip>:<port>"`
* Comma-separated list of hostnames and/or CIDRs: `"noProxy": "<hostname | CIDR>"`

You must specify key information in the `noProxy` field if you configured the following options or settings:

|===
| Configured option/setting | Required value in `noProxy` | Reason

| `httpProxy` and `httpsProxy`
| Node's CIDR
| Not specifying the node's CIDR can break the cluster.

| `cluster-registration-url`
| Host of `cluster-registration-url`
| The host information allows you to access the cluster from {rancher-product-name}.
|===

{harvester-product-name} appends necessary addresses to user-specified `noProxy` values (for example,`localhost,127.0.0.1,0.0.0.0,10.0.0.0/8,longhorn-system,cattle-system,cattle-system.svc,harvester-system,.svc,.cluster.local`). This ensures that internal traffic flows as expected.

*Example*:

[,json]
----
{
  "httpProxy": "http://my.proxy",
  "httpsProxy": "https://my.proxy",
  "noProxy": "some.internal.svc,172.16.0.0/16"
}
----

=== `log-level`

*Definition*: Log level for the host.

*Default value*: `info`

*Supported options and values*:

* `panic`: Least verbose log level
* `fatal`
* `error`
* `warn`, `warning`
* `info`
* `debug`
* `trace`: Most verbose log level

*Example*:

----
debug
----

=== `longhorn-v2-data-engine-enabled` [Experimental]

*Definition*: Setting that enables and disables the Longhorn V2 Data Engine.

When set to `true`, {harvester-product-name} automatically loads the kernel modules required by the Longhorn V2 Data Engine, and attempts to allocate 1024 × 2 MiB-sized huge pages (for example, 2 GiB of RAM) on all nodes. 

Changing this setting automatically restarts {rke2-product-name} on all nodes but does not affect running virtual machine workloads.

[TIP]
====
If you encounter error messages that include the phrase "not enough hugepages-2Mi capacity", allow some time for the error to be resolved. If the error persists, reboot the affected nodes.

To disable the Longhorn V2 Data Engine on specific nodes (for example, nodes with less processing and memory resources), go to the *Hosts* screen and add the following label to the target nodes:

* label: `node.longhorn.io/disable-v2-data-engine`
* value: `true`
====

*Default value*: `false`

*Example*:

```
true
```

=== `max-hotplug-ratio`

*Definition*: Setting that determines the default maximum amount of CPU and memory resources that are available to a running virtual machine. This ratio is multiplied by the amount of CPU and memory resources that you allocated when you created the virtual machine.

[sidebar]
Maximum available resource at runtime = (Resource allocated during virtual machine creation) x (`max-hotplug-ratio` value)

*Default value*: `4`

*Supported values*: `1` to `20`

*Example*:

The `max-hotplug-ratio` is set to `4`.

|===
| Resource | Allocated Amount | Maximum Available Amount

| CPU cores
| `1`
| `4`

| Memory (Gi)
| `2`
| `8`
|===

=== `ntp-servers`

*Definition*: NTP servers for time synchronization on nodes.

You can define NTP servers during xref:./configuration-file.adoc#_os_ntp_servers[installation] and update the addresses after installation.

[CAUTION]
====
Changes to the server address list are applied to all nodes.
====

*Default value*: ""

*Example*:

----
{
  "ntpServers": [
    "0.suse.pool.ntp.org",
    "1.suse.pool.ntp.org"
  ]
}
----

=== `overcommit-config`

*Definition*: Percentage of physical compute, memory, and storage resources that can be allocated for VM use.

Overcommitting is used to optimize physical resource allocation, particularly when VMs are not expected to fully consume the allocated resources most of the time. Setting values greater than 100% allows scheduling of multiple VMs even when physical resources are notionally fully allocated.

*Default values*: `{ "cpu":1600, "memory":150, "storage":200 }`

With the default values, it would be possible to schedule the following:

* 16x the number of physical CPUs on a host
* 1.5x the amount of physical RAM on a host
* 2x the amount of physical storage in {longhorn-product-name}

A VM that is configured to use 2 CPUs (equivalent to 2,000 milliCPU) can consume the full allocation as long as the resources are available. However, if the host is running heavy workloads and an overcommit value is set (for example, 1600%), {harvester-product-name} only requests 125 milliCPU from the Kubernetes scheduler (2000/16 = 125 milliCPU).

*Example*:

[,json]
----
{
  "cpu": 1000,
  "memory": 200,
  "storage": 300
}
----

=== `release-download-url`

*Definition*: URL for downloading the software required for upgrades.

{harvester-product-name} retrieves the ISO URL and checksum value from the `+${URL}/${VERSION}/version.yaml+` file that is accessible through the configured URL.

*Default value*: `+https://releases.rancher.com/harvester+`

*Example (version.yaml)*:

----
apiVersion: harvesterhci.io/v1beta1
kind: Version
metadata:
  name: ${VERSION}
  namespace: harvester-system
spec:
  isoChecksum: ${ISO_CHECKSUM}
  isoURL: ${ISO_URL}
----

=== `server-version`

*Definition*: Version of {harvester-product-name} that is installed on nodes.

*Example*:

----
v1.0.0-abcdef-head
----

=== `ssl-certificates`

*Definition*: SSL certificates for the UI and API.

[CAUTION]
====
Changing this setting might cause single-node clusters to temporarily become unavailable or inaccessible.
====

*Default value*: `{}`

*Example*:

[,json]
----
{
  "ca": "-----BEGIN CERTIFICATE-----\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\n-----END CERTIFICATE-----",
  "publicCertificate": "-----BEGIN CERTIFICATE-----\nSOME-CERTIFICATE-ENCODED-IN-PEM-FORMAT\n-----END CERTIFICATE-----",
  "privateKey": "-----BEGIN RSA PRIVATE KEY-----\nSOME-PRIVATE-KEY-ENCODED-IN-PEM-FORMAT\n-----END RSA PRIVATE KEY-----"
}
----

=== `ssl-parameters`

*Definition*: Enabled SSL/TLS protocols and ciphers of the UI and API.

[IMPORTANT]
.important
====
If you misconfigure this setting and are unable to access the UI and API, see xref:/troubleshooting/cluster.adoc#_i_cant_access_suse_virtualization_after_i_changed_ssltls_enabled_protocols_and_ciphers[Troubleshooting].
====

*Default value*: None

*Supported options and values*:

* `protocols`: Enabled protocols.
* `ciphers`: Enabled ciphers.

For more information about the supported options, see https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#ssl-protocols[`ssl-protocols`] and https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#ssl-ciphers[`ssl-ciphers`] in the Ingress-Nginx Controller documentation.

If you do not specify any values, {harvester-product-name} uses `TLSv1.2` and `ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305`.

*Example*:

----
{
  "protocols": "TLSv1.2 TLSv1.3",
  "ciphers": "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-CHACHA20-POLY1305"
}
----

=== `storage-network`

*Definition*: Segregated storage network for {longhorn-product-name} traffic.

By default, {longhorn-product-name} uses the management network, which is limited to a single interface and shared with cluster-wide workloads. If your implementation requires network segregation, you can use a xref:../../networking/storage-network.adoc[storage network] to isolate {longhorn-product-name} in-cluster data traffic.

[IMPORTANT]
.important
====
Shut down all VMs before configuring this setting.

Specify an IP range in the IPv4 CIDR format. The number of IPs must be four times the number of your cluster nodes.
====

*Default value*: ""

*Example*:

----
{
  "vlan": 100,
  "clusterNetwork": "storage",
  "range": "192.168.0.0/24"
}
----

=== `support-bundle-image`

*Definition*: Support bundle image, with various versions available in https://hub.docker.com/r/rancher/support-bundle-kit/tags[`rancher/support-bundle-kit`].

*Default value*: `support-bundle-kit` image that is packed into the {harvester-product-name} ISO and is specific to each {harvester-product-name} release.

*Supported options and values*:

The value is a JSON object literal that contains the following key-value pairs:

* `repository`: Name of the repository that stores the support bundle image.
+
* `tag`:  Tag assigned to the support bundle image.
+
* `imagePullPolicy`: Pull policy of the support bundle image. The supported values are `IfNotPresent`, `Always`, and `Never`. For more information, see https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy[Image pull policy] in the Kubernetes documentation.

*Example*:

----
{
  "repository": "rancher/support-bundle-kit",
  "tag": "v0.0.25",
  "imagePullPolicy": "IfNotPresent"
}
----

In this example, the default image tag of the cluster is `v0.0.25`.

The CLI shows the following `support-bundle-image` setting object:

```
apiVersion: harvesterhci.io/v1beta1
default: '{"repository":"rancher/support-bundle-kit","tag":"v0.0.25","imagePullPolicy":"IfNotPresent"}'  // default value, automatically set
kind: Setting
metadata:
  name: support-bundle-image
...
status: {}
```

After some time, a newer image tag (`v0.0.36`) is specified in the `value` field using the UI.

image::advanced/support-bundle-image-set-customized-value.png[]

```
apiVersion: harvesterhci.io/v1beta1
default: '{"repository":"rancher/support-bundle-kit","tag":"v0.0.25","imagePullPolicy":"IfNotPresent"}'
kind: Setting
metadata:
  name: support-bundle-image
...
status: {}
value: '{"repository":"rancher/support-bundle-kit","tag":"v0.0.36","imagePullPolicy":"IfNotPresent"}'  // your setting value
```

Eventually, this cluster is upgraded and the object changes again.

```
apiVersion: harvesterhci.io/v1beta1
default: '{"repository":"rancher/support-bundle-kit","tag":"v0.0.38","imagePullPolicy":"IfNotPresent"}'  // default value, automatically updated after upgrade
kind: Setting
metadata:
  name: support-bundle-image
...
status: {}
value: '{"repository":"rancher/support-bundle-kit","tag":"v0.0.36","imagePullPolicy":"IfNotPresent"}'  // your setting value is kept unchanged
```

The value of `tag` in the `value` field is `v0.0.36`, while the value of `tag` in the `default` field is `v0.0.38`.

To clear the outdated setting and use the default image tag, run the following command, remove the `value` field, and save the changes.

```
$ kubectl edit settings.harvesterhci.io support-bundle-image
```

The object appears as follows after the `value` field is removed.

```
apiVersion: harvesterhci.io/v1beta1
default: '{"repository":"rancher/support-bundle-kit","tag":"v0.0.38","imagePullPolicy":"IfNotPresent"}'
kind: Setting
metadata:
  name: support-bundle-image
...
status: {}
```

The *Use the default value* button on the UI can be used to copy the contents of the `default` field to the `value` field.

image::advanced/support-bundle-image-set-use-default-value.png[]

The object appears as follows after the changes are saved.

```
apiVersion: harvesterhci.io/v1beta1
default: '{"repository":"rancher/support-bundle-kit","tag":"v0.0.38","imagePullPolicy":"IfNotPresent"}'  // default
kind: Setting
metadata:
  name: support-bundle-image
...
status: {}
value: '{"repository":"rancher/support-bundle-kit","tag":"v0.0.38","imagePullPolicy":"IfNotPresent"}'  // copied from default
```

When the cluster is upgraded in the future, the contents of the `value` field may become outdated again because the default image tag is likely to change.

[NOTE]
====
* The value of `tag` in the `default` field is always based on the image that is packed into the {harvester-product-name} ISO. This field is automatically updated whenever the cluster is upgraded.
+
* The `default` field is used when the `value` field is not set or is left empty. {harvester-product-name} checks if the default image is stored in the cluster and is up-to-date.
+
* Configuring this setting is not required. If you decide to specify a different image tag in the `value` field, remember that this tag may become outdated when the cluster is upgraded.
+
* Use the command `$ kubectl edit settings.harvesterhci.io support-bundle-image` to clear the `value` field.
+
* The *Use the default value* button on the UI only copies the contents of the `default` field to the `value` field. You may use this to replace an outdated image tag, but the copied tag will eventually become outdated as well (when the cluster is upgraded and the `default` field is updated).
+
* If your cluster is in an air-gapped environment and you specified a non-default image tag in the `value` field, ensure that the image is available in the local containerd registry. {harvester-product-name} won't xref:/troubleshooting/cluster.adoc#_generate_a_support_bundle[generate a support bundle] if the image is not available.
====

=== `support-bundle-namespaces`

*Definition*: Additional namespaces that you can use when xref:/troubleshooting/cluster.adoc#_generate_a_support_bundle[generating a support bundle].

By default, the support bundle only collects resources from the following predefined namespaces:

* cattle-dashboards
* cattle-fleet-local-system
* cattle-fleet-system
* cattle-fleet-clusters-system
* cattle-monitoring-system
* fleet-local
* harvester-system
* local
* longhorn-system
* cattle-logging-system

Namespaces that you select are appended to the predefined namespaces list.

*Default value*: None

=== `support-bundle-timeout`

*Definition*: Number of minutes {harvester-product-name} allows for the completion of the support bundle generation process.

The process is considered to have failed when the data collection and file packing tasks are not completed within the configured number of minutes. {harvester-product-name} does not continue or retry support bundle generation processes that have timed out. When the value is `0`, the timeout feature is disabled.

*Default value*: `10`

=== `support-bundle-expiration`

*Definition*: Number of minutes {harvester-product-name} waits before deleting a support bundle that has been packaged but not downloaded (either deliberately or unsuccessfully) or retained.

You can specify a value greater than or equal to 0. When the value is 0, {harvester-product-name} uses the default value.

*Default value*: `30`

=== `support-bundle-node-collection-timeout`

*Definition*: Number of minutes {harvester-product-name} allows for collection of logs and configurations on the nodes for the support bundle.

If the collection process is not completed within the allotted time, {harvester-product-name} still allows you to download the support bundle (without the uncollected data). You can specify a value greater than or equal to 0. When the value is 0, {harvester-product-name} uses the default value.

*Default value*: `30`

=== `upgrade-checker-enabled`

*Definition*: Setting that automatically checks for available upgrades.

*Default value*: `true`

*Example*:

----
false
----

=== `upgrade-checker-url`

*Definition*: URL used to check for available upgrades.

This setting can only be used if the `upgrade-checker-enabled` setting is set to `true`.

*Default value*: `+https://harvester-upgrade-responder.rancher.io/v1/checkupgrade+`

*Example*:

----
https://your.upgrade.checker-url/v99/checkupgrade
----

=== `upgrade-config`

*Definition*: Upgrade-related configuration.

*Default value*: `{"imagePreloadOption":{"strategy":{"type":"sequential"}}, "restoreVM": false}`

*Supported options and fields*:

* `imagePreloadOption`: Options for the image preloading phase.
+
The full ISO contains the core operating system components and all required container images. {harvester-product-name} can preload these container images to each node during installation and upgrades. When workloads are scheduled to management and worker nodes, the container images are ready to use.
+
* `strategy`: Image preload strategy.
+
* `type`: Type of image preload strategy.
+
** `sequential`: The container images from the target ISO are preloaded to each node. This is the default option.
+
** `skip`: The container images are not preloaded to each node. *Do not use this option in production environments.*
+
[IMPORTANT]
====
If you decide to use `skip`, ensure that the following requirements are met:

* You have a private container registry that contains all required images.
* Your cluster has high-speed internet access and is able to pull all images from Docker Hub when necessary.

Note any potential internet service interruptions and how close you are to reaching your https://www.docker.com/increase-rate-limits/[Docker Hub rate limit]. Failure to download any of the required images may cause the upgrade to fail and may leave the cluster in a middle state.
====
+
** `parallel` (*experimental*): Nodes preload images in batches. You can adjust this using the `concurrency` option.
+
* `concurrency`: Number of nodes that can simultaneously preload images. This option takes effect only when `type` is set to `parallel`.
+
The default value is `0`, which is equivalent to following the cluster's node counts. Using `0` allows the system to dynamically follow the scale of the cluster. Values higher than the cluster's node counts are treated as `0`, while lower values are considered invalid and are rejected.
+
[NOTE]
====
{harvester-product-name} deploys an upgrade-repo service on the cluster that serves as an HTTP server for nodes that need to preload the container images. When a `concurrency` value is set, each batch of nodes downloads the container images from this upgrade-repo in parallel. Because of this, you must consider the speed of the {harvester-product-name} management network and the read speed of the default disk for {longhorn-product-name}.
====
+
* `restoreVM`: Option that enables {harvester-product-name} to automatically restore previously running non-migratable virtual machines after the upgrade is _successfully_ completed. You can specify either of the following values:
+
** `true`: {harvester-product-name} forcibly shuts down *running* and *paused* non-migratable virtual machines on each node. After the upgrade is completed, the previously running virtual machines are automatically restarted, while the paused virtual machines remain shut down.
** `false`: This is the default value. In multi-node environments, you must stop all non-migratable virtual machines to allow the upgrade to proceed. {harvester-product-name} does not restart these virtual machines after the upgrade is completed.
+
[NOTE]
====
All virtual machines are considered non-migratable in single-node environments. {harvester-product-name} forcibly shuts down all virtual machines before starting the upgrade process.
====

*Example*:

[,json]
----
{
  "imagePreloadOption": {
    "strategy": {
      "type": "parallel",
      "concurrency": 2
    }
  },
  "restoreVM": true
}
----

=== `vip-pools`

*Versions*: Deprecated as of v1.2.0 (Use xref:/networking/ip-pool.adoc[IP pools] instead.)

*Definition*: Global or namespace-specific IP address pools of the VIP by CIDR or IP range.

*Default value*: `{}`

*Example*:

[,json]
----
{
  "default": "172.16.0.0/24,172.16.1.0/24",
  "demo": "172.16.2.50-172.16.2.100,172.16.2.150-172.16.3.200"
}
----

=== `vm-force-reset-policy`

*Definition*: Setting that allows you to force rescheduling of a VM when the node that it is running on becomes unavailable.

When the state of the node changes to `Not Ready`, the VM is force deleted and rescheduled to an available node after the configured number of seconds.

When the node becomes unavailable or is powered off, the VM only restarts and does not migrate.

*Default value*: `{"enable":true, "period":300}`

*Example*:

[,json]
----
{
  "enable": "true",
  "period": 300
}
----

=== `vm-migration-network`

*Definition*: Segregated network for virtual machine migration traffic.

By default, {harvester-product-name} uses the built-in cluster network `mgmt` for virtual machine migration. `mgmt` is limited to a single interface and is shared with cluster-wide workloads. If your implementation requires network segregation, you can use a xref:networking/vm-migration-network.adoc[VM migration network] to isolate migration traffic.

[IMPORTANT]
====
Specify an IP range in the IPv4 CIDR format. The number of IP addresses must be larger than or equal to the number of your cluster nodes.
====

*Default value*: ""

*Example*:

[,json]
----
{
  "vlan": 100,
  "clusterNetwork": "vm-migration",
  "range": "192.168.1.0/24"
}
----

=== `volume-snapshot-class`

*Definition*: VolumeSnapshotClassName for the VolumeSnapshot and VolumeSnapshotContent when restoring a VM to a namespace that does not contain the source VM.

*Default value*: `longhorn`

*Example*:

`longhorn`

== UI Settings

=== `branding`

*Definition*: Setting allows you to globally rebrand the UI by customizing the product name, logos, and color scheme.

*Default value*: *Harvester*

[CAUTION]
====
Because this setting is part of the `settings.management.cattle.io` custom resource, you cannot configure any of the supported options (for example, `Logo` and `Primary Color`) using a xref:./configuration-file.adoc#_system_settings[configuration file].
====

image::advanced/branding.png[containerd-registry]

*Supported options and values*:

* *Private Label*: Product name or other text that replaces "Harvester" in most locations on the UI.
* *Logo*: Logo image in the top-level navigation header. You must upload logos for both light and dark modes.
* *Favicon*: Small image displayed next to the page title in the browser tab.
* *Primary Color*: Main color used throughout the UI.
* *Link Color*: Color used for link text throughout the UI.

=== `ui-index`

*Definition*: HTML index location for the UI.

*Default value*: `+https://releases.rancher.com/harvester-ui/dashboard/latest/index.html+`

*Example*:

----
https://your.static.dashboard-ui/index.html
----

=== `ui-path`

*Definition*: Path that describes the location of `index.html`, which is used to access the UI.

`ui-path` serves as the entry point to the UI and is active only in the following situations:

* The value of `ui-source` is `bundled`.
* The value of `ui-source` is `auto`, but `ui-index` is unable to retrieve the HTML file.

*Default value*: `/usr/share/harvester/harvester`

*Examples*:

`index.html` is stored in a container in `/home/samplefolder`. The value of `ui-source` is `bundled`.

Scenario 1: The value of `ui-path` is `/home/samplefolder`. Whenever you access the UI, the content of `/home/samplefolder/index.html` is displayed.

Scenario 2: The value of `ui-index` points to a page that is unavailable or non-existent (for example, `notexist-example.com/index.html`). When you access the UI for the first time, the content of `/home/samplefolder/index.html` is displayed. However, if you modify the `ui-index` setting to use the default value and access the UI again, the content of `/home/samplefolder/index.html` is still displayed (even if the new `ui-index` value points to an available page). For more information, see https://github.com/harvester/harvester/issues/6066[Issue #6066].

=== `ui-plugin-index`

*Definition*: JavaScript address for the plugin (when accessing {harvester-product-name} from {rancher-product-name}).

*Default value*: `+https://releases.rancher.com/harvester-ui/plugin/harvester-latest/harvester-latest.umd.min.js+`

*Example*:

----
https://your.static.dashboard-ui/*.umd.min.js
----

=== `ui-source`

*Definition*: Setting that allows you to configure how to load the UI source.

*Default value*: `auto`

*Supported values*:

* `auto`: Automatically detects whether to use the bundled UI or not.
* `external`: Uses the external UI source.
* `bundled`: Uses the bundled UI source.

*Example*:

----
external
----