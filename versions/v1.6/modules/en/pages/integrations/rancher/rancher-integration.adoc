= {rancher-product-name} Integration
:revdate: 2025-09-18
:page-revdate: {revdate}

https://documentation.suse.com/cloudnative/rancher-manager[{rancher-product-name}] is an open-source multi-cluster management platform. {rancher-short-name} has integrated {harvester-product-name} by default to centrally manage virtual machines and containers.

You can import and manage multiple {harvester-product-name} clusters using {rancher-short-name}'s xref:./virtualization-management.adoc[Virtualization Management] feature, which leverages {rancher-short-name}'s https://documentation.suse.com/cloudnative/rancher-manager/v2.11/en/rancher-admin/users/authn-and-authz/authn-and-authz.html[authentication] feature and https://documentation.suse.com/cloudnative/rancher-manager/v2.11/en/rancher-admin/users/authn-and-authz/manage-role-based-access-control-rbac/manage-role-based-access-control-rbac.html[RBAC control] for xref:./virtualization-management.adoc#_multi_tenancy[multi-tenancy] support.

Before performing any deployment steps, check the xref:../../installation-setup/requirements.adoc#_network_requirements[network requirements].

+++<div class="text-center">++++++<iframe width="950" height="475" src="https://www.youtube.com/embed/fyxDm3HVwWI" title="YouTube video player" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">++++++</iframe>++++++</div>+++

image::rancher/virtualization-management.png[virtualization-management]

== Deploying the {rancher-short-name} server

To use {rancher-short-name} with {harvester-product-name}, you must install {rancher-short-name} on a separate server. If you want to try the integration features, you can create a virtual machine in {harvester-product-name} and install the {rancher-short-name} server by following the https://documentation.suse.com/cloudnative/rancher-manager/v2.11/en/installation-and-upgrade/quick-start/deploy-rancher/helm-cli.html[Helm CLI Quick Start].

For production environments, you can https://documentation.suse.com/cloudnative/rancher-manager/v2.11/en/installation-and-upgrade/quick-start/deploy-rancher/deploy-rancher.html[deploy {rancher-short-name} and provision a Kubernetes cluster] in various cloud providers.

If you want to run {rancher-short-name} on-premises or with a provider not listed in the documentation, you can https://documentation.suse.com/cloudnative/rancher-manager/v2.11/en/installation-and-upgrade/other-installation-methods/http-proxy/install-rancher.html#_install_the_helm_cli[install the Helm CLI] and then install {rancher-short-name} using Helm.

[CAUTION]
====
*Do not install {rancher-short-name} with Docker in production*. Otherwise, your environment may be damaged, and your cluster may not be able to be recovered. Installing {rancher-short-name} in Docker should only be used for quick evaluation and testing purposes.
====

== Virtualization management

With {rancher-short-name}'s virtualization management feature, you can import and manage your {harvester-product-name} cluster. By clicking one of the imported clusters, you can easily access and manage a range of {harvester-product-name} cluster resources, including hosts, virtual machines, images, volumes, and more. Additionally, the virtualization management feature leverages {rancher-short-name}'s existing capabilities, such as authentication with various auth providers and multi-tenancy support.

For in-depth insights, please refer to the xref:./virtualization-management.adoc[virtualization management] page.

image::rancher/import-harvester-cluster.png[import-cluster]

== Creating Kubernetes clusters using the Harvester node driver

You can launch a Kubernetes cluster from {rancher-short-name} using the xref:../../integrations/rancher/node-driver/node-driver.adoc[Harvester node driver]. When {rancher-short-name} deploys Kubernetes onto these nodes, choose RKE2.

One benefit of installing Kubernetes on node pools hosted by the node driver is that if a node loses connectivity with the cluster, {rancher-short-name} can automatically create another node to join the cluster to ensure that the count of the node pool is as expected.

The Harvester node driver is included in {rancher-short-name} by default. For more information, see xref:../../integrations/rancher/node-driver/node-driver.adoc[Node Driver].

image::rancher/harvester-node-driver.png[harvester-node-driver]

== Bare-metal container workload support (Experimental)

{harvester-product-name} enables you to deploy and manage container workloads directly to the underlying {harvester-product-name} cluster. With this feature, you can seamlessly combine the power of virtual machines with the flexibility of containerization, allowing for a more versatile and efficient infrastructure setup.

image::rancher/harvester-container-dashboard.png[harvester-container-dashboard]

This guide will walk you through enabling and using this experimental feature, highlighting its capabilities and best practices.

To enable this new feature flag, follow these steps:

. Click the hamburger menu and choose the *Global Settings* tab.
. Click *Feature Flags* and locate the new feature flag `harvester-baremetal-container-workload`.
. Click the drop-down menu and select *Activate* to enable this feature.
. If the feature state changes to *Active*, the feature is successfully enabled.

image::rancher/harvester-baremetal-container-workload-feature.png[harvester-baremetal-container-workload-feature]

=== Key features

* *Unified Dashboard View*: Once you've enabled the feature, you can explore the dashboard view of the {harvester-product-name} cluster, just like you would with other standard Kubernetes clusters. This unified experience simplifies the management and monitoring of both your virtual machines and container workloads from a single, user-friendly interface.

* *Deploy Custom Workloads*: This feature lets you deploy custom container workloads directly to the bare-metal {harvester-product-name} cluster. While this functionality is experimental, it introduces exciting possibilities for optimizing your infrastructure. However, we recommend deploying container and virtual machine workloads in separate namespaces to ensure clarity and separation.

[NOTE]
====
* Critical system components such as monitoring, logging, {rancher-short-name}, KubeVirt, and Longhorn are all managed by the {harvester-product-name} cluster itself. You can't upgrade or modify these components. Therefore, exercise caution and avoid making changes to these critical system components.
* It is essential not to deploy any workloads to the system namespaces `cattle-system`, `harvester-system`, or `longhorn-system`. Keeping your workloads in separate namespaces is crucial to maintaining clarity and preserving the integrity of the system components.
* Deploying container and virtual machine workloads in separate namespaces is recommended.
====

== {fleet-product-name} support (Experimental)

You can leverage https://documentation.suse.com/cloudnative/continuous-delivery/index.html[{fleet-product-name}] for managing container workloads and configuring {harvester-product-name} with a GitOps-based approach.

[IMPORTANT]
====
The {rancher-short-name} feature `harvester-baremetal-container-workload` must be enabled.
====

. On the {rancher-short-name} UI, go to *â˜° -> Continuous Delivery*.
+
image::rancher/continuous-delivery-overview.png[]

. (Optional) On the *Clusters* tab, edit the Fleet cluster config to add labels that can be used to group {harvester-product-name} clusters.
+
In this example, the label `location=private-dc` was added.
+
image::rancher/fleet-cluster-config.png[]
+
image::rancher/fleet-additional-labels.png[]

. (Optional) On the *Cluster Groups* tab, create a cluster group.
+
In this example, the cluster group `private-dc-clusters` is created with a cluster selector rule that matches the label key/value pair of `location=private-dc`.
+
image::rancher/create-cluster-group.png[]

. On the *Git Repos* tab, create a Git repository named `harvester-config` that points to the https://github.com/harvester/harvester-fleet-examples[harvester-fleet-examples repository], with the branch defined as `main`. You must define the following paths:
+
* `keypair`
* `vmimage`
* `vmnetwork`
* `cloudinit`
+
image::rancher/gitrepo-definition.png[]

. Click *Next*, and then define the Git repository targets. You can select all clusters, an individual cluster, or a group of clusters.
+
In this example, the cluster group named `private-dc-clusters` is used.
+
image::rancher/gitrepo-targets.png[]

. Click *Save*. It may take a few seconds for the resources to be rolled out to the target clusters.
+
image::rancher/gitrepo-synced.png[]
